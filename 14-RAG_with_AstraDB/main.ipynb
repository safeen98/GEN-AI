{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6541fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APICommander about to raise from: [{'message': \"Collection already exists: trying to create Collection ('sample_db') with different settings\", 'errorCode': 'EXISTING_COLLECTION_DIFFERENT_SETTINGS', 'id': 'd4031ab4-25aa-48f7-b858-4c4252a2acb3', 'family': 'REQUEST', 'title': 'Collection already exists', 'scope': 'EMPTY'}]\n",
      "/Users/mohdsafeenkhan/Desktop/Machine_Learning/GEN-AI/venv/lib/python3.11/site-packages/langchain_astradb/utils/astradb.py:448: UserWarning: Astra DB collection 'sample_db' is detected as having indexing turned on for all fields (either created manually or by older versions of this plugin). This implies stricter limitations on the amount of text each string in a document can store. Consider indexing anew on a fresh collection to be able to store longer texts. See https://github.com/langchain-ai/langchain-datastax/blob/main/libs/astradb/README.md#warnings-about-indexing for more details.\n",
      "  if not self._validate_indexing_policy(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is self attention',\n",
       " 'context': [Document(id='5da45ed8a42841a7879d96b06691233f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mohdsafeenkhan/Desktop/Machine_Learning/GEN-AI/15-RAG_With_AstraDB/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range'),\n",
       "  Document(id='36c3b925c0374944aac2b8998e3f41da', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mohdsafeenkhan/Desktop/Machine_Learning/GEN-AI/14-RAG_with_AstraDB/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range'),\n",
       "  Document(id='010549fab66347b7b62e7eb916a61d47', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mohdsafeenkhan/Desktop/Machine_Learning/GEN-AI/15-RAG_With_AstraDB/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range'),\n",
       "  Document(id='cab4e78a34774b6d9a40280059cd45ce', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/mohdsafeenkhan/Desktop/Machine_Learning/GEN-AI/15-RAG_With_AstraDB/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].')],\n",
       " 'answer': 'Self-attention is an attention mechanism that relates different positions of a single sequence in order to compute a representation of the sequence. It is used to learn dependencies between positions in the sequence.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import cassio\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['ASTRA_DB_APPLICATION_TOKEN']=os.getenv('ASTRA_DB_APPLICATION_TOKEN')\n",
    "os.environ['ASTRA_DB_API_ENDPOINT'] = os.getenv('ASTRA_DB_API_ENDPOINT')\n",
    "os.environ['HF_TOKEN'] =os.getenv('HF_TOKEN')\n",
    "llm = ChatGroq(model='llama-3.3-70b-versatile')\n",
    "embedding = HuggingFaceEndpointEmbeddings(model= 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "loader = PyPDFLoader('/Users/mohdsafeenkhan/Desktop/Machine_Learning/GEN-AI/14-RAG_with_AstraDB/attention.pdf')\n",
    "doc = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap = 400)\n",
    "doc_splitted = splitter.split_documents(doc)\n",
    "\n",
    "astra_vector_db = AstraDBVectorStore(embedding=embedding,collection_name='sample_db')\n",
    "\n",
    "\n",
    "astra_vector_db.add_documents(doc_splitted)\n",
    "\n",
    "\n",
    "astra_db_retriver = astra_vector_db.as_retriever()\n",
    "\n",
    "\n",
    "ChatPrompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant , provide the best answer you can ONLY WITH THE context provided , if you do not know the answer just say The context is not enough\n",
    "<context>\n",
    "{context}\n",
    "<context>\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm,ChatPrompt)\n",
    "retrival_chain = create_retrieval_chain(astra_db_retriver,qa_chain)\n",
    "\n",
    "\n",
    "response = retrival_chain.invoke(\n",
    "    {'input':'what is self attention'}\n",
    ")\n",
    "response\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
