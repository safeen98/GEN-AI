{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c8ee5b",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b73f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9704cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model = \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579740b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Safeen Khan, nice to meet you. It's great that you work at Accenture, a leading global professional services company. What do you do at Accenture, if you don't mind me asking? Are you working in a specific department or project?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 48, 'total_tokens': 102, 'completion_time': 0.149819304, 'prompt_time': 0.002193019, 'queue_time': 0.0561167, 'total_time': 0.152012323}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4181589c-4195-43db-9c29-2a1e1743249e-0', usage_metadata={'input_tokens': 48, 'output_tokens': 54, 'total_tokens': 102})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hello my name is Safeen Khan and I work at Accenture\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd84358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Safeen Khan, and you work at Accenture.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 128, 'total_tokens': 143, 'completion_time': 0.032296024, 'prompt_time': 0.006322502, 'queue_time': 0.056734068, 'total_time': 0.038618526}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c374e504-f69c-4bbf-8f33-5d6b72529ff6-0', usage_metadata={'input_tokens': 128, 'output_tokens': 15, 'total_tokens': 143})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello my name is Safeen Khan and I work at Accenture\"),\n",
    "        AIMessage(content = \"Hello Safeen Khan! It's nice to meet you. Accenture is a well-known and respected company, so that's great that you're a part of their team. What do you do at Accenture, if you don't mind me asking? Are you working on any exciting projects or initiatives?\"),\n",
    "        HumanMessage(content = \"What is my name and where do I work\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98c639",
   "metadata": {},
   "source": [
    "We can see the model has some history and can remember the content that we have told"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83242b9b",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38440eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id : str) ->BaseChatMessageHistory:    #BaseChatMessageHistory is the return type\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a8b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\n",
    "    \"session_id\" : \"chat1\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2f1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hello my name is Safeen Khan and I work at Accenture\")],\n",
    "    config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a602aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Safeen Khan, nice to meet you. It's great that you work at Accenture, a renowned global consulting firm. What do you do at Accenture, if I may ask? Are you part of a specific practice or team, such as technology, strategy, or operations? I'm here to help and chat, so feel free to share more about your work or anything else you'd like to discuss.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6caaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Safeen Khan.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name ?\")],\n",
    "    config = config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca8ead",
   "metadata": {},
   "source": [
    "Now if we change the config or main the session_id we will se it doen't remembers my name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b35fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = {\"configurable\":{\n",
    "    \"session_id\":\"chat2\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bf3de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your name. I'm a large language model, I don't have personal interactions or memories, and I don't retain any information about individual users. Each time you interact with me, it's a new conversation. If you'd like to share your name with me, I'd be happy to chat with you!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name ?\")],\n",
    "    config = config2\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93119af4",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e4aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate , MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are the world best system design engineer answer my question accordingly\"),\n",
    "        MessagesPlaceholder(variable_name='message')\n",
    "    ]\n",
    ")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d82d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The pub-sub model, short for Publish-Subscribe model, is a design pattern that allows for the decoupling of producers and consumers of data. In this model, publishers (or producers) send messages to a topic or channel, and subscribers (or consumers) listen to these topics or channels to receive the messages.\\n\\nHere\\'s a breakdown of the pub-sub model:\\n\\n**Components:**\\n\\n1. **Publisher** (or Producer): The entity that sends messages to a topic or channel.\\n2. **Subscriber** (or Consumer): The entity that listens to a topic or channel to receive messages.\\n3. **Topic** (or Channel): The medium through which messages are sent and received.\\n4. **Broker** (or Message Queue): The intermediary that manages the topics or channels and routes messages from publishers to subscribers.\\n\\n**How it works:**\\n\\n1. A publisher sends a message to a topic or channel.\\n2. The broker receives the message and stores it in the topic or channel.\\n3. Subscribers listen to the topic or channel and receive the message from the broker.\\n4. The subscriber can then process the message as needed.\\n\\n**Example:**\\n\\nLet\\'s consider a simple example of a weather forecasting system:\\n\\n* **Publisher**: A weather station (e.g., \"WeatherStation1\") that sends temperature updates to a topic called \"TemperatureUpdates\".\\n* **Topic**: \"TemperatureUpdates\" is the topic where temperature updates are published.\\n* **Subscribers**: A mobile app (e.g., \"WeatherApp\") and a website (e.g., \"WeatherWebsite\") that subscribe to the \"TemperatureUpdates\" topic to receive temperature updates.\\n* **Broker**: A message broker (e.g., Apache Kafka, RabbitMQ) that manages the \"TemperatureUpdates\" topic and routes temperature updates from the weather station to the mobile app and website.\\n\\nHere\\'s a step-by-step example:\\n\\n1. The weather station (\"WeatherStation1\") sends a temperature update (e.g., \"25°C\") to the \"TemperatureUpdates\" topic.\\n2. The message broker (e.g., Apache Kafka) receives the temperature update and stores it in the \"TemperatureUpdates\" topic.\\n3. The mobile app (\"WeatherApp\") and website (\"WeatherWebsite\") are subscribed to the \"TemperatureUpdates\" topic and receive the temperature update from the broker.\\n4. The mobile app and website can then display the updated temperature to their users.\\n\\n**Benefits:**\\n\\n1. **Decoupling**: The publisher and subscribers are decoupled, meaning that the publisher doesn\\'t need to know about the subscribers, and the subscribers don\\'t need to know about the publisher.\\n2. **Scalability**: The pub-sub model allows for horizontal scaling, as new subscribers can be added without affecting the publisher.\\n3. **Flexibility**: The pub-sub model supports multiple messaging patterns, such as point-to-point, publish-subscribe, and request-response.\\n\\nThe pub-sub model is widely used in many applications, including:\\n\\n* Message queues (e.g., Apache Kafka, RabbitMQ)\\n* Event-driven architectures (e.g., serverless computing)\\n* Real-time data processing (e.g., streaming analytics)\\n* IoT (Internet of Things) applications\\n\\nI hope this explanation helps! Do you have any specific questions or scenarios you\\'d like me to elaborate on?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 670, 'prompt_tokens': 56, 'total_tokens': 726, 'completion_time': 1.393496734, 'prompt_time': 0.002741467, 'queue_time': 0.051594083, 'total_time': 1.396238201}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a7494b21-734e-4aa8-85a5-6b9af480b84f-0', usage_metadata={'input_tokens': 56, 'output_tokens': 670, 'total_tokens': 726})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    'message':[HumanMessage(content=\"What is pub-sub model , explain with example\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b2024b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Data Transfer in Pub/Sub Model**\\n\\nIn the Pub/Sub model, data transfer occurs when a subscriber receives a notification from the topic. The notification typically includes a message or payload that contains the data to be transferred. Here\\'s a step-by-step explanation of how data transfer works in the Pub/Sub model:\\n\\n1. **Publisher sends a message**: The publisher sends a message to the topic, which includes the data to be transferred.\\n2. **Message is stored in the topic**: The message is stored in the topic, and the topic notifies the subscribers that a new message is available.\\n3. **Subscriber receives notification**: The subscriber receives a notification that a new message is available in the topic.\\n4. **Subscriber requests the message**: The subscriber sends a request to the topic to retrieve the message.\\n5. **Message is delivered to the subscriber**: The topic delivers the message to the subscriber, which includes the data to be transferred.\\n\\n**Using AWS SNS**\\n\\nWhen using AWS SNS, the message that is sent to the topic is limited to 256 KB. If you need to transfer larger amounts of data, you have a few options:\\n\\n### Option 1: Use a Pointer to the Data\\n\\nInstead of sending the data itself, you can send a pointer to the data, such as an Amazon S3 object key or a URL. The subscriber can then retrieve the data from the specified location.\\n\\n**Example:**\\n```json\\n{\\n  \"data_url\": \"https://example-bucket.s3.amazonaws.com/data.txt\"\\n}\\n```\\n### Option 2: Use a Message Payload with a Reference to the Data\\n\\nYou can send a message with a reference to the data, such as a database ID or a file ID. The subscriber can then use this reference to retrieve the data from the relevant system.\\n\\n**Example:**\\n```json\\n{\\n  \"data_id\": 12345\\n}\\n```\\n### Option 3: Use a Separate Data Transfer Mechanism\\n\\nYou can use a separate data transfer mechanism, such as Amazon SQS or Amazon Kinesis, to transfer the data. The message sent to the SNS topic can then include a reference to the data, such as a message ID or a stream name.\\n\\n**Example:**\\n```json\\n{\\n  \"message_id\": \"12345-67890\"\\n}\\n```\\n**Code Example**\\n\\nHere\\'s an example of how you might implement data transfer using AWS SNS and a separate data transfer mechanism:\\n```python\\nimport boto3\\nimport json\\n\\nsns = boto3.client(\\'sns\\')\\ns3 = boto3.client(\\'s3\\')\\n\\n# Publisher code\\ndef publish_message(data):\\n    message = {\\n        \\'data_url\\': f\\'https://{s3.meta.region_name}.s3.amazonaws.com/my-bucket/{data[\"file_name\"]}\\'\\n    }\\n    response = sns.publish(\\n        TopicArn=\\'arn:aws:sns:REGION:ACCOUNT_ID:TOPIC_NAME\\',\\n        Message=json.dumps(message)\\n    )\\n\\n# Subscriber code\\ndef handle_message(event):\\n    message = json.loads(event[\\'Records\\'][0][\\'Sns\\'][\\'Message\\'])\\n    data_url = message[\\'data_url\\']\\n    # Retrieve the data from S3\\n    response = s3.get_object(Bucket=\\'my-bucket\\', Key=data_url.split(\\'/\\')[-1])\\n    data = response[\\'Body\\'].read()\\n    # Process the data\\n```\\nNote that this is just a high-level overview, and the specific implementation details will depend on your use case and requirements.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 708, 'prompt_tokens': 772, 'total_tokens': 1480, 'completion_time': 1.202122486, 'prompt_time': 0.062121838, 'queue_time': 0.048321182, 'total_time': 1.2642443239999999}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--08924159-4371-496f-b220-0548beca518a-0', usage_metadata={'input_tokens': 772, 'output_tokens': 708, 'total_tokens': 1480})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)\n",
    "config = {'configurable':{'session_id':'chat3'}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content='In pub sub model how is data transfered , like if we are using aws sns , they just send a message on topic , but what if we want some data to be transfered when that subscriber is called')],\n",
    "    config = config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba1207",
   "metadata": {},
   "source": [
    "Adding more complexity to the model by making two variable as dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffc5d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicPrompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',\"You are a world class language translator , translate the following sentense in {language}\"),\n",
    "    MessagesPlaceholder(variable_name='messages')\n",
    "])\n",
    "chain2 = dynamicPrompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2602a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='मेरा नाम मोहम्मद सफीन खान है, मैं एक्सेंचर में जनरल एआई इंजीनियर के रूप में काम करता हूँ।', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 105, 'total_tokens': 152, 'completion_time': 0.136284942, 'prompt_time': 0.005207611, 'queue_time': 0.052621189, 'total_time': 0.141492553}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--aa7b9051-89e2-4c42-807e-b3f920b3c908-0', usage_metadata={'input_tokens': 105, 'output_tokens': 47, 'total_tokens': 152})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain2,get_session_history,input_messages_key = 'messages')\n",
    "config = {'configurable':{'session_id':'chat4'}}\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"My name is Mohd Safeen Khan , I work as Gen AI engineer at Accenture\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af643cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
